# -*- coding: utf-8 -*-
"""academia_redfit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0g_II4t22-mB2SYbX0GUNHgr5to9-G8
"""

from google.colab import files
import io
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Upload do arquivo
uploaded = files.upload()

# Verificar qual nome o arquivo realmente tem
print("Arquivos disponíveis após upload:")
print(uploaded.keys())

# Pegar o nome real do arquivo (pode ser diferente do esperado)
file_name = list(uploaded.keys())[0]  # Pega o primeiro arquivo da lista
print(f"\nUsando arquivo: {file_name}")

# Ler o arquivo com o nome correto
df = pd.read_csv(io.BytesIO(uploaded[file_name]))

print("\nColunas disponíveis:")
print(df.columns.tolist())
print("\nPrimeiras 5 linhas:")
print(df.head())
print("\nInformações do dataset:")
print(df.info())

# Verificar valores nulos
print("\nValores nulos por coluna:")
print(df.isnull().sum())

# Verificar coluna alvo (ajuste conforme o dataset real)
possible_targets = ['estado', 'status', 'nivel_atividade', 'situacao', 'ativo', 'cancelou']
target_column = None

for col in possible_targets:
    if col in df.columns:
        target_column = col
        break

if target_column is None:
    # Se não encontrar nenhuma das colunas sugeridas, use a última
    target_column = df.columns[-1]
    print(f"\nNenhuma coluna alvo óbvia encontrada. Usando '{target_column}' como alvo")

print(f"\nVariável alvo selecionada: '{target_column}'")

# Separar features e target
X = df.drop(columns=[target_column])
y = df[target_column]

# Codificar a variável alvo
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nVariável alvo '{target_column}' transformada:")
for i, class_name in enumerate(le.classes_):
    print(f"  {class_name} -> {i}")

# Identificar colunas numéricas e categóricas
numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

print(f"\nColunas numéricas: {numeric_cols}")
print(f"Colunas categóricas: {categorical_cols}")

# Pré-processamento
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Dividir os dados
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Criar e treinar o modelo
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))
])

model.fit(X_train, y_train)

# Fazer previsões
y_pred = model.predict(X_test)

# Avaliar o modelo
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAcurácia: {accuracy:.4f}")
print("\nRelatório de classificação:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# CORREÇÃO: Obter os nomes das features corretamente após o pré-processamento
# Primeiro, ajustamos o preprocessor em todos os dados para obter os nomes corretos
preprocessor.fit(X)

# Obter os nomes das features após o pré-processamento
feature_names = []

# Para features numéricas
if numeric_cols:
    feature_names.extend(numeric_cols)

# Para features categóricas (após one-hot encoding)
if categorical_cols:
    # Obter o transformer categórico
    cat_transformer = preprocessor.named_transformers_['cat']
    # Obter o onehot encoder
    onehot_encoder = cat_transformer.named_steps['onehot']
    # Obter os nomes das features one-hot encoded
    onehot_features = onehot_encoder.get_feature_names_out(categorical_cols)
    feature_names.extend(onehot_features)

# Obter as importâncias das features do modelo
importances = model.named_steps['classifier'].feature_importances_

# Verificar se os tamanhos são iguais
print(f"\nNúmero de feature names: {len(feature_names)}")
print(f"Número de importances: {len(importances)}")

# Se ainda houver discrepância, usar apenas as importances com índices
if len(feature_names) != len(importances):
    print("Ajustando para tamanhos iguais...")
    # Usar apenas o número correto de feature names
    feature_names = [f'feature_{i}' for i in range(len(importances))]
else:
    print("Tamanhos compatíveis!")

# Criar o DataFrame com as importâncias
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTop 10 features mais importantes:")
print(feature_importance_df.head(10))

# Visualização das features mais importantes
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
top_features = feature_importance_df.head(10)
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel('Importância')
plt.title('Top 10 Features Mais Importantes')
plt.gca().invert_yaxis()  # Para mostrar a mais importante no topo
plt.show()